{"cells":[{"cell_type":"code","source":["\"\"\"\nAuthor: Santiago Morante\nDISTRIBUTED ANOMALY DETECTION ALGORITHM BASED ON DISSIMILARITY MAPPING FILTERING\n\"\"\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["###################################################################################\n###########################   CLASS SIGNAL ########################################\n###################################################################################\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import DataFrame as DFPyspark\nfrom pyspark.rdd import RDD\nfrom pandas import DataFrame as DFPandas\n\nclass Signal():\n  \"\"\"\n  SIGNAL: \n  The input data must be in regular table style (rows, columns): \n    1. First, it is converted into RDD (if not yet)\n    2. Every element is cast to string (if categorical) or float (if numerical) and missing values are imputed\n    3. Every row is indexed with the index in the first position of a tuple (index, listOfValuesOfTheRow)\n  \n  Each tuple in a Signal instance represents an indexed vector of data.\n  \n  :param typeOfData:           Defines type of data: \"categorical\" or \"numerical\"\n  :param missingNumDefault:    Number to insert in missing values for numerical datasets\n  :param missingCatDefault:    string to insert in missing values for categorical datasets\n  \n  Use case:\n    >>> data=sqlContext.read.table(\"your_table\")\n    >>> signals = Signal(typeOfData=\"numerical\").create(data)\n  \"\"\"\n\n  def __init__(self, typeOfData=\"numerical\", missingNumDefault=0, missingCatDefault=\"NA\"):\n    \"\"\"Initialize parameters\"\"\"\n    self.typeOfData = typeOfData\n    self.missingNumDefault = missingNumDefault\n    self.missingCatDefault = missingCatDefault\n      \n  def create(self, dataset):\n    \"\"\"Creates Signal from list, rdd, dataframe (spark) or dataframe (pandas)\"\"\"\n    rdd = self.convertToRDD(dataset).map(self.imputeMissingValues)\n    return self.indexRDD(rdd)\n    \n  def convertToRDD(self, dataset):\n    \"\"\"Converts dataset into RDD (if not yet)\"\"\"\n    sc = SparkContext.getOrCreate()\n    if isinstance(dataset, RDD):\n      return dataset\n    elif isinstance(dataset, DFPyspark):\n        return dataset.rdd.map(list)\n    elif isinstance(dataset, DFPandas):\n      sqlContext = SQLContext.getOrCreate(sc)\n      return sqlContext.createDataFrame(dataset).rdd.map(list)\n    else:\n      try:\n          return sc.parallelize(dataset)\n      except:\n          raise TypeError(\"convertToRDD cannot convert dataset because it is not in a recognized format!\")\n  \n  def imputeMissingValues(self, line):\n    \"\"\"Imputes default value to missing values in each line of RDD\"\"\"\n    if self.typeOfData == \"numerical\":\n      return [float(x) if x else self.missingNumDefault for x in line]\n    elif self.typeOfData == \"categorical\":\n      return [str(x) if x else self.missingCatDefault for x in line]\n    else:\n      raise NameError(\"typeOfData not recognized\")\n      \n  def indexRDD(self, rdd):\n    \"\"\"Index each row of a RDD as a tuple (index, listOfValuesOfTheRow)\"\"\"\n    return rdd.zipWithIndex().map(lambda x: (x[1],x[0]))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["###################################################################################\n###########################   CLASS DMF   #########################################\n###################################################################################\nfrom fastdtw import fastdtw\nfrom numpy.linalg import norm\nfrom numpy import array, count_nonzero\n\nclass DMF():\n    \"\"\"\n    DISSIMILARITY MAPPING FILTERING:\n    S. Morante, J. G. Victores and C. Balaguer, \"Automatic demonstration and feature selection for robot learning,\" \n    Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th International Conference on, Seoul, 2015, pp. 428-433.\n    doi: 10.1109/HUMANOIDS.2015.7363569\n    URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7363569&isnumber=7362951\n    \n    :param dissimilarityMethod:  Method for performing dissimilarity: euclidean (default), dtw, hamming\n    :param mappingMethod:        Method for performing mapping: sum (default)\n    :param filteringMethod:      Method for performing filtering: zscore (default), threshold\n    :param alpha:                Threshold used in filtering step (zero by default)\n\n    Use case (assuming \"signals\" is in correct format):\n      >>> model= DMF(dissimilarityMethod=\"euclidean\", filteringMethod=\"zscore\")\n      >>> print (\"Index, totalValue: \", model.detect(signals).collect())    \n    \"\"\"\n   \n    def __init__(self, dissimilarityMethod=\"euclidean\", mappingMethod=\"sum\", filteringMethod=\"zscore\", alpha=0):\n      \"\"\"Initialize parameters\"\"\"\n      self.dissimilarityMethod = dissimilarityMethod\n      self.mappingMethod = mappingMethod\n      self.filteringMethod = filteringMethod\n      self.alpha = alpha\n\n    def detect(self, signals):\n      \"\"\"Detects anomalies in the dataset (must be in format (index,listOfValues))\"\"\"\n      #dissimilarity\n      dataDissimilarity = signals.cartesian(signals).map(self.dissimilarity)\n      #mapping\n      dataMapping = dataDissimilarity.reduceByKey(self.mapping)\n      #filtering\n      if self.filteringMethod == \"zscore\":\n        self.mappingMean  = dataMapping.values().mean()\n        self.mappingStDev = dataMapping.values().stdev()\n      dataFiltering = dataMapping.filter(self.filtering)\n      return dataFiltering\n    \n    def dissimilarity(self, doublePairs): \n      \"\"\"DISSIMILARITY: calculate distance between elements\"\"\"\n      if self.dissimilarityMethod == \"euclidean\":\n        return doublePairs[0][0], norm(array(doublePairs[1][1])-array(doublePairs[0][1]))\n      elif self.dissimilarityMethod == \"dtw\":\n        distance, path = fastdtw(doublePairs[0][1], doublePairs[1][1])\n        return doublePairs[0][0], distance\n      elif self.dissimilarityMethod == \"hamming\":\n        return doublePairs[0][0], count_nonzero(doublePairs[0][1] != doublePairs[1][1])\n      else:\n        raise NameError(\"dissimilarityMethod not recognized\")\n\n    def mapping(self, a,b):\n      \"\"\"MAPPING: reduce the comparisons matrix to single value per element key\"\"\"\n      if self.mappingMethod == \"sum\":\n        return a+b\n      else:\n        raise NameError(\"mappingMethod not recognized\")\n        \n    def filtering(self, pairs):\n      \"\"\"FILTERING: filtering data by threshold (alpha) using Z-score\"\"\"\n      if self.filteringMethod == \"zscore\":\n        try:\n          return (pairs[1] - self.mappingMean)/float(self.mappingStDev) > self.alpha \n        except:\n          print(\"[WARNING] Filtering Z-score division by zero (StDev=0)!  (all values are equal)\")\n          return []\n      elif self.filteringMethod == \"threshold\":\n        return pairs[1] > self.alpha\n      else:\n        raise NameError(\"filteringMethod not recognized\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#######################################################\n### MAIN ##############################################\n#######################################################\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nimport numpy as np\n\n#context\nsc = SparkContext.getOrCreate()\n\n#get data from table\n#sqlContext = SQLContext.getOrCreate(sc)\n#data=sqlContext.read.table(\"your_table_data\")\n\n#generate data (a sine wave with 3 outliers in 10, 20 and 30)\ndataSerial = np.sin(range(0,10)).tolist() + [5] + \\\nnp.sin(range(11,20)).tolist() + [19] + \\\nnp.sin(range(21,30)).tolist() + [20] + \\\nnp.sin(range(31,50)).tolist()\ndata = sc.parallelize(np.array(dataSerial).reshape(-1,1))    \n\n#create signal\nsignals = Signal(typeOfData=\"numerical\").create(data)\n\n#create model\nmodel= DMF(dissimilarityMethod=\"euclidean\", filteringMethod=\"zscore\")\n\n#detect anomalies using DMF algorithm\nprint (\"Index, totalValue: \", model.detect(signals).collect())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"anomaly_detection","notebookId":1029771884125740},"nbformat":4,"nbformat_minor":0}
