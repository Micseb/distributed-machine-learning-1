{"cells":[{"cell_type":"code","source":["\"\"\"\nAuthor: Santiago Morante\nDISTRIBUTED ANOMALY DETECTION ALGORITHM BASED ON DISSIMILARITY MAPPING FILTERING\nS. Morante, J. G. Victores and C. Balaguer, \"Automatic demonstration and feature selection for robot learning,\" \nHumanoid Robots (Humanoids), 2015 IEEE-RAS 15th International Conference on, Seoul, 2015, pp. 428-433.\ndoi: 10.1109/HUMANOIDS.2015.7363569\nURL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7363569&isnumber=7362951\n\"\"\"\n\nfrom pyspark.sql import DataFrame as DFPyspark\nfrom pyspark.sql import SQLContext\nfrom pyspark.rdd import RDD\nfrom pyspark import SparkContext\nfrom pandas import DataFrame as DFPandas\nfrom numpy.linalg import norm\nfrom numpy import array\nfrom fastdtw import fastdtw\n\n\nclass Signal():\n  \"\"\"\n  Signal:\n  1. Takes data in regular table style (rows, columns), \n  2. Converts it to RDD (if not yet)\n  3. Indexes every row with the index in the first position of a tuple (index, listOfValuesOfTheRow),\n  4. Converts every element to float and imputes missing values\n  \n  Each tuple in a Signal instance represents a vector of data.\n  \"\"\"\n\n  def __init__(self, typeOfData=\"numerical\", missingNumDefault=0, missingCatDefault=\"NA\"):\n        \"\"\"Initialize parameters\"\"\"\n        self.typeOfData = typeOfData\n        self.missingNumDefault = missingNumDefault\n        self.missingCatDefault = missingCatDefault\n      \n  def create(self, dataset):\n    \"\"\"Creates Signal from list, rdd, dataframe (spark) or dataframe (pandas)\"\"\"\n    rdd = self.convertToRDD(dataset).map(self.imputeMissingValues)\n    return self.indexRDD(rdd)\n    \n  def convertToRDD(self, dataset):\n    \"\"\"Converts dataset into RDD (if not yet)\"\"\"\n    sc = SparkContext.getOrCreate()\n    if isinstance(dataset, RDD):\n      return dataset\n    elif isinstance(dataset, DFPyspark):\n        return dataset.rdd.map(list)\n    elif isinstance(dataset, DFPandas):\n      sqlContext = SQLContext.getOrCreate(sc)\n      return sqlContext.createDataFrame(dataset).rdd.map(list)\n    else:\n      try:\n          return sc.parallelize(dataset)\n      except TypeError:\n          print(\"convertToRDD cannot convert dataset because it is not one of the allowed types \\\n          (RDD, dataframe (sparkSQL) or dataframe (pandas))\")\n  \n  def imputeMissingValues(self, line):\n    \"\"\"Imputes default value to missing values in each line of RDD\"\"\"\n    if self.typeOfData == \"numerical\":\n      return [float(x) if x else self.missingNumDefault for x in line]\n    elif self.typeOfData == \"categorical\":\n      return [str(x) if x else self.missingCatDefault for x in line]\n    else:\n      raise ValueError(\"typeOfData not recognized\")\n  \n  def indexRDD(self, rdd):\n    \"\"\"Index each row of a RDD as a tuple (index, listOfValuesOfTheRow)\"\"\"\n    return rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n  \n\n#######################################################################  \n\nclass DMF():\n    \"\"\"\n    DISSIMILARITY MAPPING FILTERING:\n    S. Morante, J. G. Victores and C. Balaguer, \"Automatic demonstration and feature selection for robot learning,\" \n    Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th International Conference on, Seoul, 2015, pp. 428-433.\n    \n    :param data:                 RDD of data points\n    :param dissimilarityMethod:  Method for performing dissimilarity\n    :param mappingMethod:        Method for performing mapping\n    :param filteringMethod:      Method for performing filtering\n    :param theta:                Threhold used in filtering step\n    \"\"\"\n   \n    def __init__(self, dissimilarityMethod=\"euclidean\", mappingMethod=\"sum\", filteringMethod=\"zscore\", theta=0):\n      \"\"\"Initialize parameters\"\"\"\n      self.dissimilarityMethod = dissimilarityMethod\n      self.mappingMethod = mappingMethod\n      self.filteringMethod = filteringMethod\n      self.theta = theta\n\n    def dissimilarity(self, doublePairs): \n      \"\"\"DISSIMILARITY: calculate distance between elements\"\"\"\n      if self.dissimilarityMethod == \"euclidean\":\n        return doublePairs[0][0], norm(array(doublePairs[1][1])-array(doublePairs[0][1]))\n      if self.dissimilarityMethod == \"dtw\":\n        distance, path = fastdtw(doublePairs[0][1], doublePairs[1][1])\n        return doublePairs[0][0], distance\n      else:\n        raise ValueError(\"dissimilarityMethod not recognized\")\n\n    def mapping(self, a,b):\n      \"\"\"MAPPING: reduce the comparisons matrix to single value per element key\"\"\"\n      if self.mappingMethod == \"sum\":\n        return a+b\n      else:\n        raise ValueError(\"mappingMethod not recognized\")\n        \n    def filtering(self, pairs, meanData, stdevData, theta):\n      \"\"\"FILTERING: filtering data by threshold (theta) using Z-score\"\"\"\n      if self.filteringMethod == \"zscore\":\n        try:\n          return (pairs[1] - meanData)/float(stdevData) > theta \n        except:\n          raise ValueError(\"Filtering Z-score division by zero (StDev=0)!  (all values are equal)\")\n      else:\n        raise ValueError(\"filteringMethod not recognized\")\n\n    def detect(self, signals):\n      \"\"\"Detects anomalies in the dataset (must be in format (index,listOfValues))\"\"\"\n      #dissimilarity\n      dataDissimilarity = signals.cartesian(signals).map(self.dissimilarity)\n      #mapping\n      dataMapping = dataDissimilarity.reduceByKey(self.mapping)\n      #filtering\n      meanData= dataMapping.values().mean()\n      stdevData= dataMapping.values().stdev()\n      dataFiltering = dataMapping.filter(lambda pairs: self.filtering(pairs, meanData, stdevData, self.theta))\n      return dataFiltering"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#######################################################\n### MAIN ##############################################\n#######################################################\n\nfrom numpy import sin, linspace\nimport matplotlib.pylab as plt\nfrom pyspark import SparkContext\n\n#sparkContext\nsc = SparkContext.getOrCreate()\nsqlContext = SQLContext.getOrCreate(sc)\ndata=sqlContext.read.table(\"dim3medium\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#create model\nmodel= DMF(dissimilarityMethod=\"euclidean\")\nsignals = Signal().create(data)\n\n#detect using model\nprint (\"Index, totalValue: \", model.detect(signals).collect())"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"DMF","notebookId":1029771884125740},"nbformat":4,"nbformat_minor":0}
