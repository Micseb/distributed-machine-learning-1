{"cells":[{"cell_type":"code","source":["\"\"\"\nAuthor: Santiago Morante\n\"\"\"\n\nfrom pyspark.sql import DataFrame as DFPyspark\nfrom pyspark.rdd import RDD\nfrom pyspark.sql import SQLContext\nfrom pyspark import SparkContext\nfrom pandas import DataFrame as DFPandas\nfrom numpy.linalg import norm\n\n\nclass DMF():\n    \"\"\"\n    DISTRIBUTED ANOMALY DETECTION ALGORITHM BASED ON DISSIMILARITY MAPPING FILTERING\n    S. Morante, J. G. Victores and C. Balaguer, \"Automatic demonstration and feature selection for robot learning,\" \n    Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th International Conference on, Seoul, 2015, pp. 428-433.\n    doi: 10.1109/HUMANOIDS.2015.7363569\n    URL: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7363569&isnumber=7362951\n\n    :param data:                 RDD of data points\n    :param dissimilarityMethod:  Method for performing dissimilarity\n    :param mappingMethod:        Method for performing mapping\n    :param filteringMethod:      Method for performing filtering\n    :param theta:                Threhold used in filtering step\n    \"\"\"\n   \n    def __init__(self, dissimilarityMethod=\"euclidean\", mappingMethod=\"sum\", filteringMethod=\"zscore\", theta=0):\n      self.dissimilarityMethod = dissimilarityMethod\n      self.mappingMethod = mappingMethod\n      self.filteringMethod = filteringMethod\n      self.theta=theta\n\n    def convertToRDD(self, dataset):\n      sc = SparkContext.getOrCreate()\n      if isinstance(dataset, RDD):\n        return dataset\n      \n      elif isinstance(dataset, DFPyspark):\n          return dataset.rdd.flatMap(list)\n      \n      elif isinstance(dataset, DFPandas):\n        sqlContext = SQLContext.getOrCreate(sc)\n        return sqlContext.createDataFrame(dataset).rdd.flatMap(list)\n      \n      else:\n        try:\n            return sc.parallelize(dataset)\n        except TypeError:\n            print(\"convertToRDD cannot convert your dataset because it is not one of the allowed types (RDD, dataframe (sql) or dataframe(pandas))\")\n \n      \n    def dissimilarity(self, doublePairs): \n      \"\"\"DISSIMILARITY: calculate distance between elements\"\"\"\n      if self.dissimilarityMethod == \"euclidean\":\n        return doublePairs[0][0], norm(doublePairs[0][1]-doublePairs[1][1])\n      else:\n        raise ValueError(\"dissimilarityMethod should be an allowed method, \"\n                            \"but got %s.\" % str(dissimilarityMethod))\n\n    def mapping(self, a,b):\n      \"\"\"MAPPING: reduce the comparisons matrix to single value per element key\"\"\"\n      if self.mappingMethod == \"sum\":\n        return a+b\n      else:\n        raise ValueError(\"mappingMethod should be an allowed method, \"\n                              \"but got %s.\" % str(mappingMethod))\n\n    def filtering(self, pairs, meanData, stdevData, theta):\n      \"\"\"FILTERING: filtering data by threshold (theta) using Z-score\"\"\"\n      if self.filteringMethod == \"zscore\":\n        return (pairs[1] - meanData)/float(stdevData) > theta \n      else:\n         raise ValueError(\"filteringMethod should be an allowed method, \"\n                              \"but got %s.\" % str(filteringMethod))\n\n    def detect(self, dataset):\n      \"\"\"Detects anomalies in the dataset\"\"\"\n \n      #convert to RDD\n      rdd = self.convertToRDD(dataset)\n      \n      #index data\n      dataIndex = rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n      \n      #dissimilarity\n      dataDissimilarity = dataIndex.cartesian(dataIndex).map(self.dissimilarity)\n      \n      #mapping\n      dataMapping = dataDissimilarity.reduceByKey(self.mapping)\n      \n      #filtering\n      meanData= dataMapping.values().mean()\n      stdevData= dataMapping.values().stdev()\n      dataFiltering = dataMapping.filter(lambda pairs: self.filtering(pairs, meanData, stdevData, self.theta))\n      \n      #return\n      return dataFiltering\n  "],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#######################################################\n### MAIN ##############################################\n#######################################################\n\nfrom numpy import sin, linspace\nimport matplotlib.pylab as plt\nfrom pyspark import SparkContext\n\n#sparkContext\nsc = SparkContext.getOrCreate()\n\n#DATA: generate sine wave with outliers\ndata = sin(range(0,10)).tolist() + \\\n                    [35] +  \\\n                    sin(range(11,20)).tolist() + \\\n                    [32]  +  \\\n                    sin(range(21,30)).tolist() + \\\n                    [60]  +  \\\n                    sin(range(31,50)).tolist() \n\n#plot\nx = linspace(0, len(data))\nfig, ax = plt.subplots()\nplt.plot(x, data, '-', linewidth=2)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#parallelize data\ndataParallel = sc.parallelize(data)    \n\n#create model\nmodel= DMF()\n\n#detect using model\nprint (\"Index, totalValue: \", model.detect(data).collect())"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"DMF","notebookId":1029771884125740},"nbformat":4,"nbformat_minor":0}
